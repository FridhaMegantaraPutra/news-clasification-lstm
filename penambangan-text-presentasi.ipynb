{"metadata":{"interpreter":{"hash":"518d3c34f07d4859b3a299cbbcd051f62aa88deb9e11988845b649988ec53799"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8442239,"sourceType":"datasetVersion","datasetId":5029734},{"sourceId":8447554,"sourceType":"datasetVersion","datasetId":5033769},{"sourceId":8454743,"sourceType":"datasetVersion","datasetId":5038886},{"sourceId":8722009,"sourceType":"datasetVersion","datasetId":5233886}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:41.045347Z","iopub.execute_input":"2024-06-27T00:56:41.045688Z","iopub.status.idle":"2024-06-27T00:56:43.374563Z","shell.execute_reply.started":"2024-06-27T00:56:41.045660Z","shell.execute_reply":"2024-06-27T00:56:43.373717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Membuat list kosong untuk menyimpan data\n# judul = []\n# tanggal = []\n# isi = []\n# url_list = []\n# kategori_list = []","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:43.376275Z","iopub.execute_input":"2024-06-27T00:56:43.376738Z","iopub.status.idle":"2024-06-27T00:56:43.380807Z","shell.execute_reply.started":"2024-06-27T00:56:43.376712Z","shell.execute_reply":"2024-06-27T00:56:43.379729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import requests\n# from bs4 import BeautifulSoup\n# import pandas as pd\n\n# # Fungsi untuk mengambil data dari halaman web Detik.com\n# def get_data(url, kategori):\n#     # Mengirim permintaan HTTP ke url\n#     response = requests.get(url)\n#     # Mengecek status kode\n#     if response.status_code == 200:\n#         # Mengubah konten web menjadi objek BeautifulSoup\n#         soup = BeautifulSoup(response.content, \"html.parser\")\n#         # Mencari elemen yang berisi data berita\n#         articles = soup.find_all(\"article\", class_=\"list-content__item\")\n#         # Melakukan iterasi untuk setiap artikel\n#         for article in articles:\n#             # Mendapatkan link artikel\n#             link = article.find(\"a\")[\"href\"]\n#             # Mengirim permintaan HTTP ke link artikel\n#             article_response = requests.get(link)\n#             # Mengecek status kode\n#             if article_response.status_code == 200:\n#                 # Mengubah konten artikel menjadi objek BeautifulSoup\n#                 article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n#                 # Mendapatkan judul artikel\n#                 title_element = article_soup.find(\"h1\", class_=\"detail__title\")\n#                 title = title_element.text.strip() if title_element else \"Title Not Found\"\n#                 # Mendapatkan tanggal artikel\n#                 date_element = article_soup.find(\"div\", class_=\"detail__date\")\n#                 date = date_element.text.strip() if date_element else \"Date Not Found\"\n#                 # Mendapatkan isi artikel\n#                 content_element = article_soup.find(\"div\", class_=\"detail__body-text\")\n#                 content = content_element.text.strip() if content_element else \"Content Not Found\"\n#                 # Menambahkan data ke list\n#                 judul.append(title)\n#                 tanggal.append(date)\n#                 isi.append(content)\n#                 url_list.append(link)\n#                 kategori_list.append(kategori)\n#                 # Mencetak judul artikel\n#                 print(title)\n#             else:\n#                 # Mencetak pesan error jika status kode tidak 200\n#                 print(f\"Error: {article_response.status_code}\")\n#     else:\n#         # Mencetak pesan error jika status kode tidak 200\n#         print(f\"Error: {response.status_code}\")\n\n# # Membuat list url dan kategori yang akan di-crawl\n# base_urls = [\"https://sport.detik.com/indeks\", \"https://health.detik.com/berita-detikhealth/indeks\", \"https://travel.detik.com/travel-news/indeks\",\"https://www.detik.com/edu/indeks\", \"https://oto.detik.com/indeks\",\" https://food.detik.com/indeks\",\"https://finance.detik.com/indeks\"]\n# categories = [\"Olahraga\", \"Kesehatan\", \"Pariwisata\", \"Pendidikan\", \"Otomotif\",'Kuliner','Keuangan']\n\n# # Inisialisasi list untuk menyimpan data\n# judul = []\n# tanggal = []\n# isi = []\n# url_list = []\n# kategori_list = []\n\n# # Melakukan iterasi untuk setiap url dan kategori\n# for base_url, category in zip(base_urls, categories):\n#     # Looping untuk beralih halaman\n#     for page in range(0, 50):\n#         url = f\"{base_url}/{page}\"\n#         # Memanggil fungsi get_data\n#         get_data(url, category)\n\n# # Membuat dataframe dari list data\n# df = pd.DataFrame({\"judul\": judul, \"tanggal\": tanggal, \"isi\": isi, \"url\": url_list, \"kategori\": kategori_list})\n\n# # Menyimpan dataframe ke file csv\n# df.to_csv(\"data_data_berita_detik.csv\", index=False)\n\n# # Menampilkan dataframe\n# print(df)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:43.382036Z","iopub.execute_input":"2024-06-27T00:56:43.382317Z","iopub.status.idle":"2024-06-27T00:56:43.395361Z","shell.execute_reply.started":"2024-06-27T00:56:43.382295Z","shell.execute_reply":"2024-06-27T00:56:43.394454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/news-clasification-dataset/data_data_berita_detik (1).csv',encoding=\"ISO-8859-1\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:43.396517Z","iopub.execute_input":"2024-06-27T00:56:43.396831Z","iopub.status.idle":"2024-06-27T00:56:43.954838Z","shell.execute_reply.started":"2024-06-27T00:56:43.396803Z","shell.execute_reply":"2024-06-27T00:56:43.953694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:43.957514Z","iopub.execute_input":"2024-06-27T00:56:43.957820Z","iopub.status.idle":"2024-06-27T00:56:43.980806Z","shell.execute_reply.started":"2024-06-27T00:56:43.957795Z","shell.execute_reply":"2024-06-27T00:56:43.979781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n# Plotting\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x='kategori', palette='inferno')\nplt.title('Jumlah Data Berita per Kategori')\n\nplt.xlabel('Kategori')\nplt.ylabel('Jumlah Data')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:43.981973Z","iopub.execute_input":"2024-06-27T00:56:43.982373Z","iopub.status.idle":"2024-06-27T00:56:44.380783Z","shell.execute_reply.started":"2024-06-27T00:56:43.982339Z","shell.execute_reply":"2024-06-27T00:56:44.379751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_nan_count = df.isna().sum().sum()\nprint(f\"Number of NaN values in feature array: {feature_nan_count}\")\n\n# Checking for NaN values in labels\nlabel_nan_count = df.isna().sum().sum()\nprint(f\"Number of NaN values in label array: {label_nan_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:44.382011Z","iopub.execute_input":"2024-06-27T00:56:44.382317Z","iopub.status.idle":"2024-06-27T00:56:44.396643Z","shell.execute_reply.started":"2024-06-27T00:56:44.382291Z","shell.execute_reply":"2024-06-27T00:56:44.395620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport pickle\n\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n\ndef preprocess_text(text):\n\n    stemmer = PorterStemmer()\n    \n\n    words = word_tokenize(text)\n    \n    \n    words = [word.lower() for word in words]\n    \n\n    words = [word for word in words if word.isalnum()]\n    \n  \n    words = [word for word in words if not word.isdigit()]\n\n    stop_words = set(stopwords.words('indonesian'))\n    \n  \n    custom_stopwords = [\"kondisi\",\"memiliki\",\"indonesia\",\"jakarta\",\"orang\",\"gamba video\",\"simak video\",\"content\",\"video 20detik\",\"video\",\"20detik\",\"simak\",\"gamba\",\"sendiri\", \"yang\", \"atau\", \"dengan\", \"untuk\", \"baca\", \"juga\", \"advertisement\",\"continue\",\"scroll\",\"dari\"]\n    stop_words.update(custom_stopwords)\n    \n    words = [word for word in words if word not in stop_words]\n    \n    # Perform stemming\n    stemmed_words = [stemmer.stem(word) for word in words]\n    with open('stemmer.pickle', 'wb') as f:\n        pickle.dump(stemmer, f)\n\n    with open('stopwords.pickle', 'wb') as f:\n        pickle.dump(stop_words, f)\n    \n    # Join the words back into a single string\n    return ' '.join(stemmed_words)\n    # Save stemmer and stop_words as pickle\n\n\n# Apply the preprocessing function to the 'isi' column\ndf['isi'] = df['isi'].apply(preprocess_text)\n\n# Print the processed DataFrame\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:56:44.397695Z","iopub.execute_input":"2024-06-27T00:56:44.397984Z","iopub.status.idle":"2024-06-27T00:58:18.545726Z","shell.execute_reply.started":"2024-06-27T00:56:44.397960Z","shell.execute_reply":"2024-06-27T00:58:18.544810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:18.547049Z","iopub.execute_input":"2024-06-27T00:58:18.547420Z","iopub.status.idle":"2024-06-27T00:58:18.560851Z","shell.execute_reply.started":"2024-06-27T00:58:18.547386Z","shell.execute_reply":"2024-06-27T00:58:18.559617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ncategories = df['kategori'].unique()\nfor category in categories:\n    # Filter the DataFrame for the current category\n    category_df = df[df['kategori'] == category]\n    \n    # Combine all 'isi' texts for the current category\n    combined_text = ' '.join(category_df['isi'])\n    \n    # Create the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='black').generate(combined_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(f'Word Cloud for Category: {category}')\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:18.562110Z","iopub.execute_input":"2024-06-27T00:58:18.562498Z","iopub.status.idle":"2024-06-27T00:58:40.241193Z","shell.execute_reply.started":"2024-06-27T00:58:18.562456Z","shell.execute_reply":"2024-06-27T00:58:40.240143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kategori = pd.get_dummies(df.kategori)\ndf = pd.concat([df, kategori], axis=1)\ndf = df.drop(columns='kategori')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:40.242662Z","iopub.execute_input":"2024-06-27T00:58:40.243237Z","iopub.status.idle":"2024-06-27T00:58:40.267413Z","shell.execute_reply.started":"2024-06-27T00:58:40.243192Z","shell.execute_reply":"2024-06-27T00:58:40.266494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = df['isi']\nlabel = df[['Kesehatan', 'Keuangan', 'Kuliner','Olahraga','Otomotif','Pariwisata','Pendidikan']].values","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:40.268644Z","iopub.execute_input":"2024-06-27T00:58:40.269003Z","iopub.status.idle":"2024-06-27T00:58:40.275351Z","shell.execute_reply.started":"2024-06-27T00:58:40.268970Z","shell.execute_reply":"2024-06-27T00:58:40.274462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Length of feature: {len(feature)}\")\nprint(f\"Length of label: {len(label)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:40.276578Z","iopub.execute_input":"2024-06-27T00:58:40.276998Z","iopub.status.idle":"2024-06-27T00:58:40.288790Z","shell.execute_reply.started":"2024-06-27T00:58:40.276965Z","shell.execute_reply":"2024-06-27T00:58:40.287828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfeature_latih, feature_test, label_latih, label_test = train_test_split(feature, label, test_size=0.3, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:40.293984Z","iopub.execute_input":"2024-06-27T00:58:40.294800Z","iopub.status.idle":"2024-06-27T00:58:40.303944Z","shell.execute_reply.started":"2024-06-27T00:58:40.294771Z","shell.execute_reply":"2024-06-27T00:58:40.303061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\npad_type = 'pre'\ntrunc_type = 'pre'\n\n\ntokenizer = Tokenizer(num_words=2000, oov_token='x')\ntokenizer.fit_on_texts(feature_latih)\ntokenizer.fit_on_texts(feature_test)\n\nsekuens_latih = tokenizer.texts_to_sequences(feature_latih)\nsekuens_test = tokenizer.texts_to_sequences(feature_test)\n\n\nmaxlen = max([len(x) for x in sekuens_latih])\n\n\n\npadded_latih = pad_sequences(sekuens_latih, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\npadded_test = pad_sequences(sekuens_test, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:40.305000Z","iopub.execute_input":"2024-06-27T00:58:40.305294Z","iopub.status.idle":"2024-06-27T00:58:56.519731Z","shell.execute_reply.started":"2024-06-27T00:58:40.305265Z","shell.execute_reply":"2024-06-27T00:58:56.518866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Tokenize the training data\n# tokenizer.fit_on_texts(feature_latih)\n\n# # Get the word to index mapping\n# word_index = tokenizer.word_index\n\n# # Print the head of the word to index mapping\n# print(\"Word to index mapping (head):\")\n# for word, index in list(word_index.items())[:5]:\n#     print(f\"Word: {word}, Index: {index}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:56.520908Z","iopub.execute_input":"2024-06-27T00:58:56.521562Z","iopub.status.idle":"2024-06-27T00:58:56.526518Z","shell.execute_reply.started":"2024-06-27T00:58:56.521532Z","shell.execute_reply":"2024-06-27T00:58:56.525502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Tokenize the training data\n# tokenizer.fit_on_texts(feature_latih)\n\n# # Encode training data sentences into sequences\n# sequences_latih = tokenizer.texts_to_sequences(feature_latih)\n\n# # Print some example sequences\n# print(\"Example sequences:\")\n# for seq in sequences_latih[:1]:  # Print only the first 5 sequences for demonstration\n#     print(seq)\n#     print(len(seq))\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:56.527681Z","iopub.execute_input":"2024-06-27T00:58:56.528027Z","iopub.status.idle":"2024-06-27T00:58:56.538963Z","shell.execute_reply.started":"2024-06-27T00:58:56.527994Z","shell.execute_reply":"2024-06-27T00:58:56.538192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Print the head of padded_latih\n# print(\"Head of padded_latih:\")\n# print(padded_latih[:1].shape)\n# print(padded_latih[:2].shape)\n# print(padded_latih[:3].shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:56.539988Z","iopub.execute_input":"2024-06-27T00:58:56.540359Z","iopub.status.idle":"2024-06-27T00:58:56.549069Z","shell.execute_reply.started":"2024-06-27T00:58:56.540311Z","shell.execute_reply":"2024-06-27T00:58:56.548216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the type of the input data\nprint(type(padded_latih))\nprint(type(label_latih))","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:56.550044Z","iopub.execute_input":"2024-06-27T00:58:56.550900Z","iopub.status.idle":"2024-06-27T00:58:56.558100Z","shell.execute_reply.started":"2024-06-27T00:58:56.550871Z","shell.execute_reply":"2024-06-27T00:58:56.557238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model tersebut adalah model jaringan saraf dalam yang dibangun menggunakan Keras. Berikut adalah penjelasan singkat dari tiap lapisan yang digunakan:\n\n1. **Embedding Layer**: `tf.keras.layers.Embedding(input_dim=2000, output_dim=8)`\n   - Layer ini mengubah input berupa integer menjadi representasi vektor berdimensi 8. `input_dim=2000` berarti ada 2000 kata unik yang dapat dimasukkan ke dalam layer ini.\n\n2. **Dropout Layer**: `tf.keras.layers.Dropout(0.0001)`\n   - Layer ini digunakan untuk mencegah overfitting dengan cara mengatur proporsi neuron yang dinonaktifkan secara acak selama pelatihan. Di sini, 0.0001 menunjukkan bahwa sangat sedikit neuron yang akan dinonaktifkan.\n\n3. **LSTM Layer**: `tf.keras.layers.LSTM(128)`\n   - Layer Long Short-Term Memory (LSTM) dengan 128 unit yang digunakan untuk menangani data urutan atau sekuensial. LSTM berguna untuk mengenali pola dalam urutan data yang panjang.\n\n4. **Dropout Layer**: `tf.keras.layers.Dropout(0.00001)`\n   - Sama seperti sebelumnya, tetapi dengan tingkat dropout yang sangat rendah.\n\n5. **Dense Layer**: `tf.keras.layers.Dense(128, activation='sigmoid')`\n   - Layer fully connected (Dense) dengan 128 neuron dan fungsi aktivasi sigmoid. Sigmoid digunakan untuk memperkenalkan non-linearitas.\n\n6. **Dropout Layer**: `tf.keras.layers.Dropout(0.00001)`\n   - Dropout layer lagi dengan tingkat yang sangat rendah.\n\n7. **Dense Layer**: `tf.keras.layers.Dense(7, activation='sigmoid')`\n   - Layer output dengan 7 neuron dan fungsi aktivasi sigmoid, biasanya digunakan untuk menghasilkan prediksi untuk masalah multi-kelas dengan 7 kelas.\n\nSecara keseluruhan, model ini terdiri dari lapisan embedding untuk kata, diikuti oleh LSTM untuk menangkap urutan data, dan beberapa lapisan dense dengan dropout untuk klasifikasi akhir menjadi 7 kelas.","metadata":{}},{"cell_type":"code","source":"# model = tf.keras.Sequential([\n#   tf.keras.layers.Embedding(input_dim=2000, output_dim=16),\n#   tf.keras.layers.Dropout(0.0001),\n#   tf.keras.layers.LSTM(128),\n#   tf.keras.layers.Dropout(0.00001),\n#   tf.keras.layers.Dense(150, activation='sigmoid'),\n#   tf.keras.layers.Dropout(0.00001),\n#   tf.keras.layers.Dense(7, activation='sigmoid')\n#   ])\n\n# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=2000, output_dim=64),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), \n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(128, activation='relu'), \n    tf.keras.layers.Dropout(0.5),  \n    tf.keras.layers.Dense(7, activation='softmax') \n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:56.559369Z","iopub.execute_input":"2024-06-27T00:58:56.559693Z","iopub.status.idle":"2024-06-27T00:58:57.504363Z","shell.execute_reply.started":"2024-06-27T00:58:56.559664Z","shell.execute_reply":"2024-06-27T00:58:57.503436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.84):\n      print(\"\\nAkurasi telah mencapai >90%!\")\n      self.model.stop_training = True\ncallbacks = myCallback()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:57.505577Z","iopub.execute_input":"2024-06-27T00:58:57.505978Z","iopub.status.idle":"2024-06-27T00:58:57.513107Z","shell.execute_reply.started":"2024-06-27T00:58:57.505945Z","shell.execute_reply":"2024-06-27T00:58:57.511993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 150\nhist = model.fit(padded_latih, label_latih, epochs=num_epochs,\n                    validation_data=(padded_test, label_test),\n                    verbose=2, callbacks=[callbacks])","metadata":{"execution":{"iopub.status.busy":"2024-06-27T00:58:57.514415Z","iopub.execute_input":"2024-06-27T00:58:57.514789Z","iopub.status.idle":"2024-06-27T01:01:13.999449Z","shell.execute_reply.started":"2024-06-27T00:58:57.514752Z","shell.execute_reply":"2024-06-27T01:01:13.998503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Mendapatkan bobot dari lapisan embedding\n# embedding_layer = model.get_layer('embedding')\n# embedding_weights = embedding_layer.get_weights()[0]\n\n# # Menampilkan embedding\n# print(\"Shape of embedding weights:\", embedding_weights.shape)\n# print(embedding_weights)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:14.001102Z","iopub.execute_input":"2024-06-27T01:01:14.001507Z","iopub.status.idle":"2024-06-27T01:01:14.005707Z","shell.execute_reply.started":"2024-06-27T01:01:14.001480Z","shell.execute_reply":"2024-06-27T01:01:14.004738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Mendapatkan bobot dari lapisan embedding\n# embedding_lstm = model.get_layer('lstm')\n# lstm_weights = embedding_lstm.get_weights()[0]\n\n# # Menampilkan embedding\n# print(\"Shape of lstm weights:\", embedding_weights.shape)\n# print(lstm_weights)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:14.006810Z","iopub.execute_input":"2024-06-27T01:01:14.007090Z","iopub.status.idle":"2024-06-27T01:01:14.018156Z","shell.execute_reply.started":"2024-06-27T01:01:14.007066Z","shell.execute_reply":"2024-06-27T01:01:14.017312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:14.019149Z","iopub.execute_input":"2024-06-27T01:01:14.019457Z","iopub.status.idle":"2024-06-27T01:01:14.052444Z","shell.execute_reply.started":"2024-06-27T01:01:14.019433Z","shell.execute_reply":"2024-06-27T01:01:14.051523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Prediksi label dari data uji\nlabel_prediksi = model.predict(padded_test)\nlabel_prediksi = [list(prediksi).index(max(prediksi)) for prediksi in label_prediksi]\n\n# Konversi one-hot encoded labels menjadi label kelas tunggal\nlabel_asli = [list(label).index(1) for label in label_test]\n\n# Buat confusion matrix\ncm = confusion_matrix(label_asli, label_prediksi)\n\n# Visualisasi confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Kesehatan', 'Keuangan', 'Kuliner','Olahraga','Otomotif','Pariwisata','Pendidikan'], yticklabels=['Kesehatan', 'Keuangan', 'Kuliner','Olahraga','Otomotif','Pariwisata','Pendidikan'])\nplt.title('Confusion Matrix')\nplt.show()\n     ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:23.348597Z","iopub.execute_input":"2024-06-27T01:01:23.349264Z","iopub.status.idle":"2024-06-27T01:01:29.594267Z","shell.execute_reply.started":"2024-06-27T01:01:23.349234Z","shell.execute_reply":"2024-06-27T01:01:29.593246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:34.156089Z","iopub.execute_input":"2024-06-27T01:01:34.156485Z","iopub.status.idle":"2024-06-27T01:01:34.453773Z","shell.execute_reply.started":"2024-06-27T01:01:34.156457Z","shell.execute_reply":"2024-06-27T01:01:34.452754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:34.724688Z","iopub.execute_input":"2024-06-27T01:01:34.725016Z","iopub.status.idle":"2024-06-27T01:01:34.975189Z","shell.execute_reply.started":"2024-06-27T01:01:34.724990Z","shell.execute_reply":"2024-06-27T01:01:34.974242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(maxlen)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:41.411996Z","iopub.execute_input":"2024-06-27T01:01:41.412729Z","iopub.status.idle":"2024-06-27T01:01:41.417326Z","shell.execute_reply.started":"2024-06-27T01:01:41.412695Z","shell.execute_reply":"2024-06-27T01:01:41.416448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prediksi\n","metadata":{}},{"cell_type":"code","source":"# # Simpan model\n# model.save(\"model_berita_classification1.h5\")\n\n# # Simpan tokenizer\n# import pickle\n# with open('tokenizer1.pickle', 'wb') as handle:\n#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# # Simpan nilai maxlen\n# with open('maxlen.txt', 'w') as f:\n#     f.write(str(maxlen))\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:51.570616Z","iopub.execute_input":"2024-06-27T01:01:51.570983Z","iopub.status.idle":"2024-06-27T01:01:51.575395Z","shell.execute_reply.started":"2024-06-27T01:01:51.570955Z","shell.execute_reply":"2024-06-27T01:01:51.574386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.models import load_model\n# import pickle\n# from nltk.corpus import stopwords\n# from nltk.stem import PorterStemmer\n# from nltk.tokenize import word_tokenize\n\n# # Load stemmer from pickle\n# with open('/kaggle/working/stemmer.pickle', 'rb') as f:\n#     stemmer = pickle.load(f)\n\n# # Load stopwords from pickle\n# with open('/kaggle/working/stopwords.pickle', 'rb') as f:\n#     stop_words = pickle.load(f)\n\n# # Now you can define your preprocess_text function with loaded stemmer and stopwords\n# def preprocess_text(text):\n#     from nltk.tokenize import word_tokenize\n\n#     # Tokenize the text\n#     words = word_tokenize(text)\n\n#     # Convert words to lowercase\n#     words = [word.lower() for word in words]\n\n#     # Remove punctuation\n#     words = [word for word in words if word.isalnum()]\n\n#     # Remove numbers\n#     words = [word for word in words if not word.isdigit()]\n\n#     # Remove stopwords\n#     words = [word for word in words if word not in stop_words]\n\n#     # Perform stemming using loaded stemmer\n#     stemmed_words = [stemmer.stem(word) for word in words]\n\n#     # Join the words back into a single string\n#     return ' '.join(stemmed_words)\n# # Load tokenizer\n# with open('/kaggle/working/tokenizer.pickle', 'rb') as handle:\n#     tokenizer = pickle.load(handle)\n\n# # Load model\n# model = load_model('/kaggle/working/model_berita_classification.h5')\n# # Load maxlen\n# with open('maxlen.txt', 'r') as f:\n#     maxlen = int(f.read())\n\n\n# # Contoh data baru yang ingin diprediksi\n# data_baru =  [\"Irving pernah memperkuat Celtics pada tahun 2017-2019. Selama itu, Irving mampu kemas rata-rata 24 poin per laga dan turut terpilih masuk dalam daftar NBA All-Star.Selepas dari Celtics, Iriving lanjutkan karier ke Brooklyn Nets. Lalu di tahun 2023, pemain berusia 32 tahun itu bergabung ke Dallas Mavericks.\"]\n# preprocessed_data = [preprocess_text(text) for text in data_baru]\n\n# # Preprocessing data baru\n# sekuens_data_baru = tokenizer.texts_to_sequences(data_baru)\n# padded_data_baru = pad_sequences(sekuens_data_baru, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n\n# # Prediksi\n# hasil_prediksi = model.predict(padded_data_baru)\n\n\n\n# # Hasil prediksi\n# print(hasil_prediksi)\n# categories = ['Kesehatan', 'Keuangan', 'Kuliner', 'Olahraga', 'Otomotif', 'Pariwisata', 'Pendidikan']\n\n# # Mengambil indeks probabilitas tertinggi\n# indeks_tertinggi = np.argmax(hasil_prediksi)\n\n# # Mendapatkan kategori sesuai dengan indeks tertinggi\n# kategori_prediksi = categories[indeks_tertinggi]\n\n# print(\"Kategori Prediksi:\", kategori_prediksi)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:02:05.030230Z","iopub.execute_input":"2024-06-27T01:02:05.030712Z","iopub.status.idle":"2024-06-27T01:02:05.039300Z","shell.execute_reply.started":"2024-06-27T01:02:05.030676Z","shell.execute_reply":"2024-06-27T01:02:05.038237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed_data","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:14.826101Z","iopub.status.idle":"2024-06-27T01:01:14.826502Z","shell.execute_reply.started":"2024-06-27T01:01:14.826314Z","shell.execute_reply":"2024-06-27T01:01:14.826330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T01:01:14.827934Z","iopub.status.idle":"2024-06-27T01:01:14.828304Z","shell.execute_reply.started":"2024-06-27T01:01:14.828108Z","shell.execute_reply":"2024-06-27T01:01:14.828121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}